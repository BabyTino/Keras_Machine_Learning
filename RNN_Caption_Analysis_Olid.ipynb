{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AICW2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a31sFIyrHaXl",
        "colab_type": "text"
      },
      "source": [
        "# COMP5623 Coursework on Image Caption Generation\n",
        "\n",
        "Starter code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81kdnnwJvTFx",
        "colab_type": "text"
      },
      "source": [
        "## Text preparation \n",
        "\n",
        "We need to build a vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXpWOFqFOXcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mounted Drive if using Colab; otherwise, your local path\n",
        "root = \"drive/My Drive/Colab Notebooks/data/Flickr8k/\" # <--- replace this with your root data directory\n",
        "caption_dir = root + \"captions/\"                       # <--- replace these too\n",
        "image_dir = root + \"images/\"                           # <---\n",
        "\n",
        "\n",
        "token_file = \"Flickr8k.token.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X3Fv34tlPWq",
        "colab_type": "code",
        "outputId": "cdf03bfc-8af7-461d-d785-b4b94d8a0685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9AkORttFoF_",
        "colab_type": "text"
      },
      "source": [
        "A helper function to read in our ground truth text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHC0y7zaOXq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_lines(filepath):\n",
        "    \"\"\" Open the ground truth captions into memory, line by line. \"\"\"\n",
        "    file = open(filepath, 'r')\n",
        "    lines = []\n",
        "\n",
        "    while True: \n",
        "        # Get next line from file until there's no more\n",
        "        line = file.readline() \n",
        "        if not line: \n",
        "            break\n",
        "        lines.append(line.strip())\n",
        "    file.close() \n",
        "    return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D86cJx2yv81K",
        "colab_type": "text"
      },
      "source": [
        "You can read all the ground truth captions (5 per image), into memory as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m-snsM2XHuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = read_lines(caption_dir + token_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IkK91ZuXNB2",
        "colab_type": "code",
        "outputId": "ed70d7ab-02a6-4992-cc42-d1e961649339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "lines[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of stairs in an entry way .',\n",
              " '1000268201_693b08cb0e.jpg#1\\tA girl going into a wooden building .',\n",
              " '1000268201_693b08cb0e.jpg#2\\tA little girl climbing into a wooden playhouse .',\n",
              " '1000268201_693b08cb0e.jpg#3\\tA little girl climbing the stairs to her playhouse .',\n",
              " '1000268201_693b08cb0e.jpg#4\\tA little girl in a pink dress going into a wooden cabin .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSiEAtiBFX1Y",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oksUJjLPwApA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocabulary(object):\n",
        "    \"\"\"Simple vocabulary wrapper which maps every unique word to an integer ID. \"\"\"\n",
        "    def __init__(self):\n",
        "        # Intially, set both the IDs and words to empty dictionaries.\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        # If the word does not already exist in the dictionary, add it\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            # Increment the ID for the next word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        # If we try to access a word in the dictionary which does not exist, return the <unk> id\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx['<unk>']\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEQtthpXwEoY",
        "colab_type": "text"
      },
      "source": [
        "Extract all the words from ```lines```, and create a list of them in a variable ```words```, for example:\n",
        "\n",
        "```words = [\"a\", \"an\", \"the\", \"cat\"... ]```\n",
        "\n",
        "No need to worry about duplicates.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9M3UWSAwAsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "from collections import Counter\n",
        "words = []\n",
        "words_split = []\n",
        "for i in range(len(lines)):\n",
        "  lines[i] = lines[i].replace('.', '') # Fullstops\n",
        "  lines[i] = lines[i].replace(',', '') # Commas\n",
        "  words.append(lines[i].lower().split())\n",
        "  words_split.append(lines[i].lower().split())\n",
        "\n",
        "words = list(itertools.chain.from_iterable(words))\n",
        "word_count = dict(Counter(words))\n",
        "\n",
        "del_list = []\n",
        "for key in word_count:\n",
        "  if word_count[key] <= 3 and 'jpg' not in key:\n",
        "    del_list.append(key)\n",
        "\n",
        "for word in words:\n",
        "  if word in del_list:\n",
        "    words.remove(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CclAtYfm6ge4",
        "colab_type": "code",
        "outputId": "a96995a5-8620-4c5c-8010-38f1e191a377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "469706"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ttUC5yQl9JH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_ids = []\n",
        "del_ids = []\n",
        "\n",
        "for i in range(len(words_split)):\n",
        "  del_ids.append(words_split[i][0])\n",
        "  img = words_split[i][0][:-5]\n",
        "  image_ids.append(img)\n",
        "\n",
        "cleaned_captions = []\n",
        "captions = words_split\n",
        "for i in range(len(captions)):\n",
        "  for del_id in del_ids:\n",
        "    if del_id in captions[i]:\n",
        "      captions[i].remove(del_id)\n",
        "  for del_word in del_list:\n",
        "    if del_word in captions[i]:\n",
        "      captions[i].remove(del_word)\n",
        "  words_join = ' '.join(captions[i])\n",
        "  cleaned_captions.append(words_join)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-g8xGOL6ABI",
        "colab_type": "code",
        "outputId": "a27af947-bc86-4464-95c7-cf7294830c12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(cleaned_captions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40455"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHBMe-ATwLIQ",
        "colab_type": "text"
      },
      "source": [
        "Build the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctwErx_ZwAzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a vocab instance\n",
        "vocab = Vocabulary()\n",
        "\n",
        "# Add the token words first\n",
        "vocab.add_word('<pad>')\n",
        "vocab.add_word('<start>')\n",
        "vocab.add_word('<end>')\n",
        "vocab.add_word('<unk>')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzEYIvJ-GA_G",
        "colab_type": "text"
      },
      "source": [
        "Add the rest of the words from the parsed captions:\n",
        "\n",
        "``` vocab.add_word('new_word')```\n",
        "\n",
        "Don't add words that appear three times or less."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj99JT2XwA4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word in words:\n",
        "  if 'jpg' not in word:\n",
        "    vocab.add_word(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10UpLqW6FQbl",
        "colab_type": "code",
        "outputId": "09133e1e-8761-4bd1-9ab5-0b488b44f7b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB30f4wYwSvg",
        "colab_type": "text"
      },
      "source": [
        "## Dataset and loaders for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raEOHrpnbbKY",
        "colab_type": "text"
      },
      "source": [
        "Keeping the same order, concatenate all the cleaned words from each caption into a string again, and add them all to a list of strings ```cleaned_captions```. Store all the image ids in a list ```image_ids```."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_FbII1VwVSg",
        "colab_type": "text"
      },
      "source": [
        "The dataframe for the image paths and captions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYQz4T3mwA2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'image_id': image_ids,\n",
        "    'path': [image_dir + image_id + \".jpg\" for image_id in image_ids],\n",
        "    'caption': cleaned_captions\n",
        "}\n",
        "\n",
        "data_df = pd.DataFrame(data, columns=['image_id', 'path', 'caption'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POB7UiJLwYsf",
        "colab_type": "code",
        "outputId": "605dd333-1d35-4204-f178-8683082344b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data_df.head(n=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>path</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000268201_693b08cb0e</td>\n",
              "      <td>drive/My Drive/Colab Notebooks/data/Flickr8k/i...</td>\n",
              "      <td>a child in a pink dress is climbing up a set o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000268201_693b08cb0e</td>\n",
              "      <td>drive/My Drive/Colab Notebooks/data/Flickr8k/i...</td>\n",
              "      <td>a girl going into a wooden building</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000268201_693b08cb0e</td>\n",
              "      <td>drive/My Drive/Colab Notebooks/data/Flickr8k/i...</td>\n",
              "      <td>a little girl climbing into a wooden playhouse</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000268201_693b08cb0e</td>\n",
              "      <td>drive/My Drive/Colab Notebooks/data/Flickr8k/i...</td>\n",
              "      <td>a little girl climbing the stairs to her playh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000268201_693b08cb0e</td>\n",
              "      <td>drive/My Drive/Colab Notebooks/data/Flickr8k/i...</td>\n",
              "      <td>a little girl in a pink dress going into a woo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                image_id  ...                                            caption\n",
              "0  1000268201_693b08cb0e  ...  a child in a pink dress is climbing up a set o...\n",
              "1  1000268201_693b08cb0e  ...                a girl going into a wooden building\n",
              "2  1000268201_693b08cb0e  ...     a little girl climbing into a wooden playhouse\n",
              "3  1000268201_693b08cb0e  ...  a little girl climbing the stairs to her playh...\n",
              "4  1000268201_693b08cb0e  ...  a little girl in a pink dress going into a woo...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNLQ0K-_weJy",
        "colab_type": "text"
      },
      "source": [
        "This is the Flickr8k class for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqf2_F6YwakD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "from nltk import tokenize\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class Flickr8k(Dataset):\n",
        "    \"\"\" Flickr8k custom dataset compatible with torch.utils.data.DataLoader. \"\"\"\n",
        "    \n",
        "    def __init__(self, df, vocab, transform=None):\n",
        "        \"\"\" Set the path for images, captions and vocabulary wrapper.\n",
        "        \n",
        "        Args:\n",
        "            df: df containing image paths and captions.\n",
        "            vocab: vocabulary wrapper.\n",
        "            transform: image transformer.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Returns one data pair (image and caption). \"\"\"\n",
        "\n",
        "        vocab = self.vocab\n",
        "\n",
        "        caption = self.df['caption'][index]\n",
        "        img_id = self.df['image_id'][index]\n",
        "        path = self.df['path'][index]\n",
        "\n",
        "        image = Image.open(open(path, 'rb'))\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert caption (string) to word ids.\n",
        "        tokens = caption.split()\n",
        "        caption = []\n",
        "        # Build the Tensor version of the caption, with token words\n",
        "        caption.append(vocab('<start>'))\n",
        "        caption.extend([vocab(token) for token in tokens])\n",
        "        caption.append(vocab('<end>'))\n",
        "        target = torch.Tensor(caption)\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vkld_4CwkPO",
        "colab_type": "text"
      },
      "source": [
        "We need to overwrite the default PyTorch ```collate_fn()``` because our ground truth captions are sequential data of varying lengths. The default ```collate_fn()``` does not support merging the captions with padding.\n",
        "\n",
        "You can read more about it here: https://pytorch.org/docs/stable/data.html#dataloader-collate-fn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5YmKr9ewkqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def caption_collate_fn(data):\n",
        "    \"\"\" Creates mini-batch tensors from the list of tuples (image, caption).\n",
        "    Args:\n",
        "        data: list of tuple (image, caption). \n",
        "            - image: torch tensor of shape (3, 256, 256).\n",
        "            - caption: torch tensor of shape (?); variable length.\n",
        "    Returns:\n",
        "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
        "        targets: torch tensor of shape (batch_size, padded_length).\n",
        "        lengths: list; valid length for each padded caption.\n",
        "    \"\"\"\n",
        "    # Sort a data list by caption length from longest to shortest.\n",
        "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "    images, captions = zip(*data)\n",
        "\n",
        "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
        "    lengths = [len(cap) for cap in captions]\n",
        "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
        "    for i, cap in enumerate(captions):\n",
        "        end = lengths[i]\n",
        "        targets[i, :end] = cap[:end]        \n",
        "    return images, targets, lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6VDx2O5FSiM",
        "colab_type": "text"
      },
      "source": [
        "Now we define the data transform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpRbVk6BFTGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Crop size matches the input dimensions expected by the pre-trained ResNet\n",
        "data_transform = transforms.Compose([ \n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),  # Why do we choose 224 x 224?\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),   # Using ImageNet norms\n",
        "                         (0.229, 0.224, 0.225))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgS9OpZ7FaAj",
        "colab_type": "text"
      },
      "source": [
        "Initialising the datasets. The only twist is that every image has 5 ground truth captions, so each image appears five times in the dataframe. We don't want an image to appear in more than one set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnTvR684GGVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unit_size = 5\n",
        "\n",
        "train_split = 0.95 # Defines the ratio of train/test data.\n",
        "\n",
        "# We didn't shuffle the dataframe yet so this works\n",
        "train_size = unit_size * round(len(data_df)*train_split / unit_size)\n",
        "\n",
        "dataset_train = Flickr8k(\n",
        "    df=data_df[:train_size].reset_index(drop=True),\n",
        "    vocab=vocab,\n",
        "    transform=data_transform,\n",
        ")\n",
        "\n",
        "dataset_test = Flickr8k(\n",
        "    df=data_df[(train_size):].reset_index(drop=True),\n",
        "    vocab=vocab,\n",
        "    transform=data_transform,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWuWg72dGOq9",
        "colab_type": "text"
      },
      "source": [
        "Write the dataloaders ```train_loader``` and ```test_loader``` - explicitly replacing the collate_fn:\n",
        "\n",
        "```train_loader = torch.utils.data.DataLoader(\n",
        "  ...,\n",
        "  collate_fn=caption_collate_fn\n",
        ")```\n",
        "\n",
        "Set train batch size to 128 and be sure to set ```shuffle=True```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkNrIRbXGLFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=128, \n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=caption_collate_fn\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=5, \n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    collate_fn=caption_collate_fn\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXlf8lt5TF0N",
        "colab_type": "text"
      },
      "source": [
        "## Encoder and decoder models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls8lyXA2GTC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet152(pretrained=True) # Pre-trained on ImageNet by default\n",
        "        layers = list(resnet.children())[:-1]      # Keep all layers except the last one\n",
        "        # Unpack the layers and create a new Sequential\n",
        "        self.resnet = nn.Sequential(*layers)\n",
        "        \n",
        "        # We want a specific output size, which is the size of our embedding, so\n",
        "        # we feed our extracted features from the last fc layer (dimensions 1 x 1000)\n",
        "        # into a Linear layer to resize\n",
        "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "        \n",
        "        # Batch normalisation helps to speed up training\n",
        "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "        \n",
        "    def forward(self, images):\n",
        "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
        "\n",
        "        with torch.no_grad():\n",
        "            features = self.resnet(images)\n",
        "        features = features.reshape(features.size(0), -1)\n",
        "        features = self.bn(self.linear(features))\n",
        "        return features\n",
        "        \n",
        "     \n",
        "        \n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
        "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        \n",
        "        # What is an embedding layer?\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # Define this layer (one at a time)\n",
        "        # self.lstm / self.rnn\n",
        "\n",
        " \n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "       \n",
        "        # self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        \n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.max_seq_length = max_seq_length\n",
        "        \n",
        "    def forward(self, features, captions, lengths):\n",
        "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
        "        embeddings = self.embed(captions)\n",
        "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
        "        # What is \"packing\" a padded sequence?\n",
        "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
        "        hiddens, _ = self.lstm(packed) # Replace with self.rnn when using RNN\n",
        "        outputs = self.linear(hiddens[0])\n",
        "        return outputs\n",
        "    \n",
        "    def sample(self, features, states=None):\n",
        "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
        "        sampled_ids = []\n",
        "        inputs = features.unsqueeze(1)\n",
        "        for i in range(self.max_seq_length):\n",
        "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
        "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
        "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
        "            sampled_ids.append(predicted)\n",
        "            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
        "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
        "        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
        "        return sampled_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9-qtkkMTrtB",
        "colab_type": "code",
        "outputId": "6c5fed45-076b-4d30-b73e-6ac56c20b79b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhecFOMRUgpe",
        "colab_type": "text"
      },
      "source": [
        "Set training parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fd2-IX2Uer3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "learning_rate = 0.001\n",
        "num_epochs = 5\n",
        "log_step = 10\n",
        "save_step = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlIwF6P8UgB4",
        "colab_type": "text"
      },
      "source": [
        "Initialize the models and set the learning parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxwDUlR2Uy7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Build the models\n",
        "encoder = EncoderCNN(embed_size).to(device)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimisation will be on the parameters of BOTH the enocder and decoder,\n",
        "# but excluding the ResNet parameters, only the new added layers.\n",
        "params = list(\n",
        "    decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters()\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(params, lr=learning_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUmSb2MHEZw3",
        "colab_type": "text"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS4oN21vNKu7",
        "colab_type": "text"
      },
      "source": [
        "The loop to train the model. Feel free to put this in a function if you prefer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRVWyb5PpgmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# images, captions, lengths = [x[0] for x in iter (test_loader).next()]\n",
        "\n",
        "def display_image(display_image):\n",
        "    tensor_image = display_image + 1\n",
        "    tensor_image = tensor_image - tensor_image.min()\n",
        "    picture = tensor_image / (tensor_image.max() - tensor_image.min())\n",
        "\n",
        "    plt.imshow(picture.permute(1, 2, 0))\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "\n",
        "def calculate_bleu(reference, candidate):\n",
        "    score = sentence_bleu(reference, candidate) \n",
        "    return score\n",
        "\n",
        "def reference_split(reference_unsplit):\n",
        "    return [sentence.split(\" \") for sentence in reference_unsplit]\n",
        "\n",
        "def candidate_split(candidate_unsplit):\n",
        "    return candidate_unsplit.split(\" \")\n",
        "\n",
        "def find_captions(caption_tensors):\n",
        "    reference_captions = []\n",
        "\n",
        "    for i in range(5):\n",
        "      reference_captions.append(get_sentence(caption_tensors[i].cpu().numpy()))\n",
        "\n",
        "    return reference_captions\n",
        "\n",
        "def get_sentence(caption_tensor): \n",
        "    # Convert word_ids to words\n",
        "    sampled_caption = []\n",
        "    for word_id in caption_tensor:\n",
        "        word = vocab.idx2word[word_id]\n",
        "        sampled_caption.append(word)\n",
        "        if word == '<end>':\n",
        "            break\n",
        "    sentence = ' '.join(sampled_caption[1:-1])\n",
        "    return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQstr2DnkEJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "sample_image = []\n",
        "sample_caption = []\n",
        "predicted_caption = []\n",
        "temp_images = []\n",
        "temp_captions = []\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "for (images, captions, lengths) in test_loader:\n",
        "\n",
        "  temp_images.append(images)\n",
        "\n",
        "  temp_captions.append(captions)\n",
        "\n",
        "sample_image = temp_images[2][2]\n",
        "sample_caption = temp_captions[2][2]\n",
        "sample_image = sample_image.to(device)\n",
        "sample_caption = sample_caption.cpu()\n",
        "\n",
        "features = encoder(images.to(device))\n",
        "wordIDs = decoder.sample(features)\n",
        "\n",
        "caption = decoder.sample(features).cpu().detach().numpy()\n",
        "for word in caption[0]:\n",
        "  predicted_caption.append(vocab.idx2word[word])\n",
        "\n",
        "# display_image(sample_image.cpu())\n",
        "\n",
        "# newcaption = np.array(wordIDs.cpu()[:1]).squeeze(0)\n",
        "\n",
        "# candidate_caption = [vocab.idx2word[word] for word in newcaption]\n",
        "# # print(\"Candidate\")\n",
        "# print(predicted_caption)\n",
        "\n",
        "# sample_caption = sample_caption.cpu().squeeze(0).tolist()\n",
        "# actual_caption = [vocab.idx2word[word] for word in sample_caption]\n",
        "# # print(\"Reference\")\n",
        "# # print(actual_caption)\n",
        "\n",
        "# score = sentence_bleu(actual_caption, sample_caption)\n",
        "# # print(\"blue_score:\\n\" + str(score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIcEuDYpLidJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# plt.imshow(images.permute(1, 2, 0)  )\n",
        "# plt.axis(\"off\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M7KY9G3NI8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoder.train()\n",
        "# decoder.train()\n",
        "# loss1 = []\n",
        "# # Train the models\n",
        "# total_step = len(train_loader)\n",
        "# for epoch in range(num_epochs):\n",
        "#     for i, (images, captions, lengths) in enumerate(train_loader):\n",
        "\n",
        "#         # Set mini-batch dataset\n",
        "#         images = images.to(device)\n",
        "#         captions = captions.to(device)\n",
        "\n",
        "#         # Packed as well as we'll compare to the decoder outputs\n",
        "#         targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
        "\n",
        "#         # Forward, backward and optimize\n",
        "#         features = encoder(images)\n",
        "#         outputs = decoder(features, captions, lengths)\n",
        "\n",
        "#         loss = criterion(outputs, targets)\n",
        "        \n",
        "#         # Zero gradients for both networks\n",
        "#         decoder.zero_grad()\n",
        "#         encoder.zero_grad()\n",
        "   \n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # Print log info\n",
        "#         if i % log_step == 0:\n",
        "#             print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "#                   .format(epoch, num_epochs, i, total_step, loss.item())) \n",
        "\n",
        "#         # If you want to save the model checkpoints - recommended once you have everything working\n",
        "#         # Make sure to save RNN and LSTM versions separately\n",
        "#         # if (i+1) % save_step == 0:\n",
        "#     loss1.append(loss.item())\n",
        "#     # torch.save(decoder.state_dict(),  'decoder{}.ckpt'.format(epoch+1))\n",
        "#     # torch.save(encoder.state_dict(),  'encoder{}.ckpt'.format(epoch+1))\n",
        "#     # Encoder = torch.load('drive/My Drive/Colab Notebooks/RNN/decoder1.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xTJLbiuBJqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(loss1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhXkaG6HOBmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# epochs = range(len(loss1))\n",
        "# nb_epochs = len(epochs)\n",
        "# plt.axis((0, 4, 2.1, 3.1))\n",
        "# plt.plot(epochs, loss1, label = 'Loss Progression')\n",
        "# plt.plot(epochs, loss1, 'o', label = 'Training loss at each Epoch')\n",
        "# plt.legend()\n",
        "# plt.draw()\n",
        "# plt.title(\"RNN Decoder\")\n",
        "# plt.pause(0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OXN8CuhR2Bq",
        "colab_type": "text"
      },
      "source": [
        "### Load Models \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbui9o2gRz9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#537559285_29be110134.jpg\n",
        "import os\n",
        "\n",
        "def load_image(image_path, transform=None):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = image.resize([224, 224], Image.LANCZOS)\n",
        "    \n",
        "    if transform is not None:\n",
        "        image = transform(image).unsqueeze(0)\n",
        "    \n",
        "    return image\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "                                transforms.ToTensor(), \n",
        "                                transforms.Normalize((0.485, 0.456, 0.406), \n",
        "                                                    (0.229, 0.224, 0.225))])\n",
        "\n",
        "all_models = os.listdir('/content/drive/My Drive/Colab Notebooks/data/RNN/')\n",
        "#all_models.sort()\n",
        "# print(all_models)\n",
        "test_encoder = EncoderCNN(embed_size).to(device)\n",
        "test_decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
        "\n",
        "# THERE ARE LSTM 12 FILES AND RNN 12 FILES IN THE GOOGL DRIVE SAVED AFTER TRAINING\n",
        "checkpoint_decoder = torch.load('/content/drive/My Drive/Colab Notebooks/data/LSTM/decoder3.ckpt') \n",
        "checkpoint_encoder = torch.load('/content/drive/My Drive/Colab Notebooks/data/LSTM/encoder3.ckpt') \n",
        "file_decoder = '/content/drive/My Drive/Colab Notebooks/data/LSTM/decoder5.ckpt'\n",
        "file_encoder = '/content/drive/My Drive/Colab Notebooks/data/LSTM/encoder5.ckpt'\n",
        "# NOW LOAD THE CHECKPOINTS TO THE MODELS\n",
        "test_encoder.load_state_dict(checkpoint_encoder)\n",
        "test_decoder.load_state_dict(checkpoint_decoder)\n",
        "\n",
        "# SETTING THE MODELS TO EVAL MODE\n",
        "test_encoder.eval()\n",
        "test_decoder.eval()\n",
        "\n",
        "test_image = load_image(image_dir + \"537559285_29be110134.jpg\", transform)\n",
        "test_image = test_image.to(device)\n",
        "\n",
        "sample_feature = test_encoder(test_image)\n",
        "sampled_ids = test_decoder.sample(sample_feature)\n",
        "sampled_ids = sampled_ids[0].cpu().numpy()\n",
        "\n",
        "test_image = Image.open(image_dir + \"537559285_29be110134.jpg\")\n",
        "\n",
        "# sampled_caption = []\n",
        "# for word_id in sampled_ids:\n",
        "#     word = vocab.idx2word[word_id]\n",
        "#     sampled_caption.append(word)\n",
        "#     if word == '<end>':\n",
        "#         break\n",
        "# sentence = ' '.join(sampled_caption[1:-1])\n",
        "\n",
        "# plt.title(sentence)\n",
        "# plt.axis(\"off\")\n",
        "# plt.imshow(test_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9VXVxWEc6GR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ref = [['the', 'little', 'girl', 'wearing', 'a', 'pink', 'hat', 'is', 'bending', 'down', 'to', 'pick', 'up', 'a', 'soccer', 'ball'], ['a', 'little', 'girl', 'in', 'a', 'pink', 'hat', 'is', 'playing', 'with', 'a', 'soccer', 'ball'], ['a', 'young', 'girl', 'is', 'playing', 'with', 'a', 'soccer', 'ball', 'in', 'the', 'grass'], ['young', 'child', 'playing', 'with', 'a', 'soccer', 'ball', 'in', 'a', 'grassy', 'area'], ['little', 'girl', 'with', 'pink', 'hat', 'playing', 'with', 'a', 'soccer', 'ball']]\n",
        "# can = ['a', 'child', 'in', 'a', 'red', 'collar', 'is', 'running', 'through', 'a', 'field']\n",
        "# # print(\"Reference Caption:\", ref)\n",
        "# print(\"Generated Caption:\", can)\n",
        "# print(\"Bleu Score:\",sentence_bleu(ref,can, weights=(0.10,0.10,0.10,0.10), smoothing_function=SmoothingFunction().method7), )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD5caDxhm0pA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ref = [['a','collie', 'is', 'running', 'through', 'an', 'obstacle', 'course'],['collie', 'jumping', 'over', 'a', 'training', 'hurdle', 'that', 'is', 'on', 'the', 'grass'],['collie', 'making', 'a', 'jump', 'over', 'a', 'yellow', 'hurdle'], ['the', 'dog', 'is', 'leaping', 'over', 'a', 'hurdle']]\n",
        "# can = ['a', 'dog', 'leaps', 'over', 'a', 'hurdle']\n",
        "# # print(\"Reference Caption:\", ref)\n",
        "# print(\"Generated Caption:\", can)\n",
        "# print(\"Bleu Score:\",sentence_bleu(ref,can, weights=(0.10,0.10,0.10,0.10), smoothing_function=SmoothingFunction().method7), )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhD11KPfXVe9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# def display(display_image):\n",
        "#     tensor_image = display_image + 1\n",
        "#     tensor_image = tensor_image - tensor_image.min()\n",
        "#     picture = tensor_image / (tensor_image.max() - tensor_image.min())\n",
        "#     # plt.imshow(np.transpose(picture[1], (1,2,0)))\n",
        "    \n",
        "\n",
        "#     plt.imshow(picture)\n",
        "#     plt.axis(\"off\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXRoz6EyVsn_",
        "colab_type": "code",
        "outputId": "09173c79-5c7d-449c-b826-4f704afa6cf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        }
      },
      "source": [
        "bleu_scores = []\n",
        "total_references=[]\n",
        "for i, (images, targets, lengths) in enumerate(test_loader): # Iterating the test data loader\n",
        "  image = images.to(device)\n",
        "  #image_features = encoder(image)\n",
        "  test_encoder.eval()\n",
        "  test_decoder.eval()\n",
        "  refs = reference_split(find_captions(targets))\n",
        "  total_references.append(refs)\n",
        "\n",
        "#test_image = load_image(image_dir + \"1007320043_627395c3d8.jpg\", transform)\n",
        "#test_image = test_image.to(device)\n",
        "  # print (\"Epoch\", file_encoder)\n",
        "  sample_feature = test_encoder(image)\n",
        "  sampled_ids = test_decoder.sample(sample_feature)\n",
        "  sampled_ids = sampled_ids[0].cpu().numpy()\n",
        "  #sampled_ids = decoder.sample(image_features) \n",
        "  #sampled_ids = sampled_ids[0].cpu().data.numpy()\n",
        "  referenced_ids = targets[0].cpu().data.numpy()\n",
        "  reference_sentence = get_sentence(referenced_ids)\n",
        "  predicted_sentence = get_sentence(sampled_ids)\n",
        "  bleu_score = sentence_bleu(refs,predicted_sentence, weights=(0.10,0.10,0.10,0.10), smoothing_function=SmoothingFunction().method7)\n",
        "  bleu_scores.append(bleu_score)\n",
        "  \n",
        "\n",
        "  # #Displaying image\n",
        "  # img = image.cpu().numpy()\n",
        "  # # display(img)\n",
        "  # plt.imshow(np.transpose(img[0], (1,2,0)))\n",
        "  # plt.axis(\"off\")\n",
        "  # plt.show()\n",
        "  \n",
        "  print(\"Bleu Score\",bleu_score)\n",
        "  if i in bleu_scores == [4]:\n",
        "    break\n",
        "  #print(\"Actual Sentence\",reference_sentence)\n",
        "  print(\"Reference:\", reference_sentence[0:])\n",
        "  # print(\"Predicted:\",predicted_sentence)\n",
        "  # print(refs)\n",
        "  # if i in img[i] <= [4]:\n",
        "    # break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bleu Score 0.6445415768239305\n",
            "Reference: a child playing on the monkey bars at a playground with an adult\n",
            "Bleu Score 0.6347773372843615\n",
            "Reference: collie jumping over a training hurdle that is on the grass\n",
            "Bleu Score 0.65223873280901\n",
            "Reference: the little girl wearing a pink hat is bending down to pick up a soccer ball\n",
            "Bleu Score 0.6492426528865304\n",
            "Reference: a little girl walking on the green grass in front of a big stone\n",
            "Bleu Score 0.6394583629945195\n",
            "Reference: two children holding hands going down a large inflatable slide\n",
            "Bleu Score 0.6478232686564145\n",
            "Reference: a woman with a green shirt takes a drink from a water fountain\n",
            "Bleu Score 0.6453631662275289\n",
            "Reference: a man is holding three things on fire in front of a child 's playground\n",
            "Bleu Score 0.6448210563693623\n",
            "Reference: a man is playing with a fire baton in the day light\n",
            "Bleu Score 0.6476027287498433\n",
            "Reference: a girl is doing the splits in the air in front of some trees\n",
            "Bleu Score 0.6413802326715403\n",
            "Reference: a man wearing a white helmet is in the water\n",
            "Bleu Score 0.645445489288514\n",
            "Reference: the boy with the red soccer suit is falling down while the boy in the white shirt has his eyes on the ball\n",
            "Bleu Score 0.6473786018242381\n",
            "Reference: a man holding a young boy so he can cross the monkey bars at the playground\n",
            "Bleu Score 0.6423218591812407\n",
            "Reference: a grey dog is bouncing a soccer ball on his head in a grass field\n",
            "Bleu Score 0.6478232686564145\n",
            "Reference: a young boy wearing blue shorts is splashing is a blue kiddie pool\n",
            "Bleu Score 0.6466290018209189\n",
            "Reference: a little child is sitting next to a beige colored dog\n",
            "Bleu Score 0.6424063802817018\n",
            "Reference: a little boy playing in the water while its and an adult while an adult holds his hand\n",
            "Bleu Score 0.649273739021558\n",
            "Reference: a man in a gray shirt and black shorts poses in front of a mountain\n",
            "Bleu Score 0.6476027287498433\n",
            "Reference: a small child with a ponytail dressed in jean shorts and a pink and brown shirt climbs on a pole near wooden playground equipment\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-245109094941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#test_image = test_image.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;31m# print (\"Epoch\", file_encoder)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0msample_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0msampled_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0msampled_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampled_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-2a0a0474d633>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    344\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    345\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 346\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qvYUg0Pzj1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #BLEU Score for entire test set\n",
        "# test_loader_iter = iter(test_loader)\n",
        "# total_references=[]\n",
        "# total_candidates=[]\n",
        "# bleu_list=[]\n",
        "# encoder.eval()\n",
        "# for batch_id, (img, cap, size) in enumerate(test_loader_iter):\n",
        "#     refs = reference_split(find_captions(cap))\n",
        "#     cads = candidate_split(generate_caption(img))\n",
        "#     bleu_list.append(sentence_bleu(refs, cads, weights=(0.50,0.35,0.10,0.05), smoothing_function=SmoothingFunction().method7))\n",
        "#     total_references.append(refs)\n",
        "#     total_candidates.append(cads)\n",
        "#     print(refs)\n",
        "#     print(cads)\n",
        "#     print(sentence_bleu(refs, cads, weights=(0.50,0.35,0.10,0.05), smoothing_function=SmoothingFunction().method7))\n",
        "# encoder.train()\n",
        "# c=[len(i) for i in total_candidates]\n",
        "\n",
        "# print(\"bleu score rnn test (0.50,0.35,0.10,0.05) is: \", corpus_bleu(total_references, total_candidates, weights=(0.50,0.35,0.10,0.05), smoothing_function=SmoothingFunction().method7) )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izXQ7BUk41Ap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i, (images, targets, lengths) in enumerate(test_loader): # Iterating the test data loader\n",
        "#   image = images.to(device)\n",
        "#   image_features = encoder(image)\n",
        "#   sampled_ids = decoder.sample(image_features) \n",
        "#   sampled_ids = sampled_ids[0].cpu().data.numpy()\n",
        "#   referenced_ids = targets[0].cpu().data.numpy()\n",
        "#   reference_sentence = get_sentence(referenced_ids)\n",
        "#   predicted_sentence = get_sentence(sampled_ids)\n",
        "#   bleu_score = sentence_bleu(reference_sentence, predicted_sentence, weights=(0.50,0.35,0.10,0.05), smoothing_function=SmoothingFunction().method7)\n",
        "  \n",
        "\n",
        "#   #Displaying image\n",
        "#   # img = image.cpu().numpy()\n",
        "#   # plt.imshow(np.transpose(img[0], (1,2,0)))\n",
        "#   # plt.show()\n",
        "\n",
        "\n",
        "#   # display_image(img)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}