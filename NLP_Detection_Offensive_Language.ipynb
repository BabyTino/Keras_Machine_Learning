{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Offensive_language_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85WHvVp8V4Yd",
        "colab_type": "text"
      },
      "source": [
        "# Loading the data and the packages / libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmDbAUXUTphh",
        "colab_type": "code",
        "outputId": "832ddfb5-3f28-45c7-8584-c2a7edcc0c68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "train_v1_id = '1QzIm1OrQYhbwY7dRtdkGLK2kFOwoSfgR'\n",
        "\n",
        "test_a_id = '17sLsGw5e2WyLWnGRqHWfbIM8fVoJPcdW'\n",
        "\n",
        "test_b_id = '11KtpkmEZMQj-PmtfyHLlxU3Ue9UbTJmp'\n",
        "\n",
        "test_c_id = '1_nizVEge67DbGrEv_cppl14FxU90CU1r'\n",
        "\n",
        "\n",
        "downloaded_train = drive.CreateFile({'id':train_v1_id}) \n",
        "downloaded_train.GetContentFile('offenseval-training-v1.tsv') \n",
        "\n",
        "downloaded_test_a = drive.CreateFile({'id':test_a_id}) \n",
        "downloaded_test_a.GetContentFile('testset-taska.tsv')\n",
        "\n",
        "downloaded_test_b = drive.CreateFile({'id':test_b_id}) \n",
        "downloaded_test_b.GetContentFile('testset-taskb.tsv') \n",
        "\n",
        "downloaded_test_c = drive.CreateFile({'id':test_c_id}) \n",
        "downloaded_test_c.GetContentFile('test_set_taskc.tsv') \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-13 14:30:56,686 : WARNING : file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
            "    from google.appengine.api import memcache\n",
            "ModuleNotFoundError: No module named 'google.appengine'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 42, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    \"file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\"\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
            "2020-05-13 14:30:56,694 : INFO : URL being requested: GET https://www.googleapis.com/discovery/v1/apis/drive/v2/rest\n",
            "2020-05-13 14:30:56,699 : INFO : Attempting refresh to obtain initial access_token\n",
            "2020-05-13 14:30:56,702 : INFO : Refreshing access_token\n",
            "2020-05-13 14:30:56,850 : INFO : URL being requested: GET https://www.googleapis.com/drive/v2/files/1QzIm1OrQYhbwY7dRtdkGLK2kFOwoSfgR?alt=json\n",
            "2020-05-13 14:30:58,634 : INFO : URL being requested: GET https://www.googleapis.com/drive/v2/files/17sLsGw5e2WyLWnGRqHWfbIM8fVoJPcdW?alt=json\n",
            "2020-05-13 14:30:59,780 : INFO : URL being requested: GET https://www.googleapis.com/drive/v2/files/11KtpkmEZMQj-PmtfyHLlxU3Ue9UbTJmp?alt=json\n",
            "2020-05-13 14:31:01,319 : INFO : URL being requested: GET https://www.googleapis.com/drive/v2/files/1_nizVEge67DbGrEv_cppl14FxU90CU1r?alt=json\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7KSLpoyTwIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Import\n",
        "!pip install -U -q GPyOpt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import spacy\n",
        "import re\n",
        "import os\n",
        "\n",
        "import logging\n",
        "from gensim.models import FastText\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import utils\n",
        "from keras.models import Model\n",
        "from keras import Sequential\n",
        "from keras.layers import Dropout, Conv1D, MaxPooling1D, LSTM,Concatenate, Dense, GlobalMaxPooling1D,GlobalAveragePooling1D, Lambda, Input, Bidirectional, GRU, concatenate, SpatialDropout1D, Reshape, merge\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "\n",
        "import GPyOpt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7irndYzfQPjl",
        "colab_type": "code",
        "outputId": "c25baece-658d-4564-be3e-f06950cf8960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "!unzip glove.twitter.27B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-13 14:31:07--  http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.twitter.27B.zip [following]\n",
            "--2020-05-13 14:31:07--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following]\n",
            "--2020-05-13 14:31:08--  http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520408563 (1.4G) [application/zip]\n",
            "Saving to: ‘glove.twitter.27B.zip.1’\n",
            "\n",
            "glove.twitter.27B.z 100%[===================>]   1.42G  1.87MB/s    in 11m 59s \n",
            "\n",
            "2020-05-13 14:43:07 (2.02 MB/s) - ‘glove.twitter.27B.zip.1’ saved [1520408563/1520408563]\n",
            "\n",
            "Archive:  glove.twitter.27B.zip\n",
            "replace glove.twitter.27B.25d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove.twitter.27B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
            "error:  invalid response [a]\n",
            "replace glove.twitter.27B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n a\n",
            "replace glove.twitter.27B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n A\n",
            "replace glove.twitter.27B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: glove.twitter.27B.200d.txt  a\n",
            "A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vh8R57QEpym",
        "colab_type": "text"
      },
      "source": [
        "# Dataset details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFGUuSrbNJRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root = \"drive/My Drive/OLIDv1/\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzRCYmAqcuWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the unlabelled dataset\n",
        "\n",
        "test_data_a = pd.read_csv(\"testset-taska.tsv\", sep = \"\\t\", header = 0)\n",
        "test_data_b = pd.read_csv(\"testset-taskb.tsv\", sep = \"\\t\", header = 0)\n",
        "test_data_c = pd.read_csv(\"test_set_taskc.tsv\", sep = \"\\t\", header = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjFl6qBkEpyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the labelled dataset\n",
        "\n",
        "data = pd.read_csv(\"offenseval-training-v1.tsv\", sep = \"\\t\", header = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbXmgyCYEpyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Dataset size:\", len(data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tcr1e3LqEpyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NANWbUuxEpy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwt-IOqnEpzB",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ordEAKsEpzD",
        "colab_type": "text"
      },
      "source": [
        "## Dataset base-cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQbweMRcEpzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load stopwords from SpaCy\n",
        "\n",
        "nlp = spacy.load(\"en\")\n",
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFtJMFsBEpzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Text preprocessing \n",
        "def clean_data(tweet):\n",
        "    splitted_tweet = tweet.lower().split()\n",
        "    clean_tweet = []\n",
        "    previous_word = None\n",
        "    user_count = 0\n",
        "    for word in splitted_tweet:\n",
        "        #if word not in spacy_stopwords:\n",
        "        word = re.sub(\"[#@]\",\"\",word)\n",
        "        word = re.sub(\"!\",\" !\",word)\n",
        "        word = re.sub(\"[?]\",\" ?\",word)\n",
        "        \n",
        "        if(word == \"user\"):\n",
        "          user_count += 1\n",
        "          \n",
        "        if(word == \"user\" and previous_word == \"user\"):\n",
        "          pass\n",
        "        else:\n",
        "          clean_tweet.append(word)\n",
        "          \n",
        "        previous_word = word\n",
        "          \n",
        "    return \" \".join(clean_tweet), user_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9AZLjK6EpzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clean data and count users\n",
        "data = data.merge(data.tweet.apply(lambda x:pd.Series({'clean':clean_data(x)[0], 'user_count': clean_data(x)[1]})), left_index=True, right_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6whnfOOrODf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalise the user count in order to have better training performance in the neural network\n",
        "max_ = data[\"user_count\"].max()\n",
        "data[\"user_count\"] = data[\"user_count\"].apply(lambda x:x/max_);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcvJ7PYXEpzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_a = data[data[\"subtask_a\"].notna()]\n",
        "data_b = data[data[\"subtask_b\"].notna()]\n",
        "data_c = data[data[\"subtask_c\"].notna()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PpV71BJEpzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_a = data_a[[\"clean\", \"user_count\"]]\n",
        "label_a = data_a[\"subtask_a\"]\n",
        "X_b = data_b[[\"clean\", \"user_count\"]]\n",
        "label_b = data_b[\"subtask_b\"]\n",
        "X_c = data_c[[\"clean\", \"user_count\"]]\n",
        "label_c = data_c[\"subtask_c\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5LM7iK6T55G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_to_number_a = {'OFF':0,'NOT':1}\n",
        "label_to_number_b = {'UNT':0,'TIN':1}\n",
        "label_to_number_c = {'IND':0,'OTH':1,'GRP':2}\n",
        "number_to_label_a = {v:k for k, v in label_to_number_a.items()}\n",
        "number_to_label_b = {v:k for k, v in label_to_number_b.items()}\n",
        "number_to_label_c = {v:k for k, v in label_to_number_c.items()}\n",
        "binary_labels_a = data_a.subtask_a.apply(lambda x:label_to_number_a[x])\n",
        "binary_labels_b = data_b.subtask_b.apply(lambda x:label_to_number_b[x])\n",
        "binary_labels_c = data_c.subtask_c.apply(lambda x:label_to_number_c[x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm2INbOYUhi_",
        "colab_type": "code",
        "outputId": "7a8ef403-3f98-4078-8694-880c91c17a22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# Tokenisation\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data.clean)\n",
        "sequences = tokenizer.texts_to_sequences(data.clean)\n",
        "seq = pad_sequences(sequences)\n",
        "\n",
        "max_seq = len(seq[0])\n",
        "print(\"maximum length of a sequence:\" ,max_seq)\n",
        "\n",
        "# Text to token\n",
        "def build_seq(dataset):\n",
        "  sequences = tokenizer.texts_to_sequences(dataset.clean)\n",
        "  seq = pad_sequences(sequences, maxlen=max_seq)\n",
        "  return seq\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maximum length of a sequence: 63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG9b5x9-Ep0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_a, X_test_a, count_train_a, count_test_a, y_train_a, y_test_a = train_test_split(build_seq(X_a), X_a.user_count, binary_labels_a, test_size=0.2)\n",
        "X_train_b, X_test_b, count_train_b, count_test_b, y_train_b, y_test_b = train_test_split(build_seq(X_b), X_b.user_count, binary_labels_b, test_size=0.2)\n",
        "X_train_c, X_test_c, count_train_c, count_test_c, y_train_c, y_test_c = train_test_split(build_seq(X_c), X_c.user_count, binary_labels_c, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbJtpbzoUXU-",
        "colab_type": "code",
        "outputId": "27dbc13d-3964-43fb-8f55-0d354693d723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "plt.figure(figsize = (12,4))\n",
        "plt.subplot(131)\n",
        "cat_a = Counter(y_train_a)\n",
        "plt.bar(list(cat_a.keys()), list(cat_a.values()))\n",
        "plt.xticks(range(len(cat_a)), label_to_number_a)\n",
        "plt.title(\"Distribition of labels in subtask A\");\n",
        "plt.subplot(132)\n",
        "cat_b = Counter(y_train_b)\n",
        "plt.bar(list(cat_b.keys()), list(cat_b.values()))\n",
        "plt.xticks(range(len(cat_b)), label_to_number_b)\n",
        "plt.title(\"Distribition of labels in subtask B\");\n",
        "plt.subplot(133)\n",
        "cat_c = Counter(y_train_c)\n",
        "plt.bar(list(cat_c.keys()), list(cat_c.values()))\n",
        "plt.xticks(range(len(cat_c)), label_to_number_c)\n",
        "plt.title(\"Distribition of labels in subtask C\");"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_CvqlHSTy7L",
        "colab_type": "text"
      },
      "source": [
        "## Over / under -spampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b8GfO0hbiLo",
        "colab_type": "text"
      },
      "source": [
        "The dataset is highly imbalanced, so we decide to combine oversampling and undersampling in order to have balanced dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmAXzTxpFxWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def same_size_data(X_train, count_train, y_train, ratio_down_over_up=0.5):\n",
        "  X_train = list(X_train)\n",
        "  count_train = list(count_train)\n",
        "  y_train = list(y_train)\n",
        "  \n",
        "  n_cat = len(Counter(y_train))\n",
        "  \n",
        "  sorted_counter = Counter(y_train).most_common()\n",
        "  max_cat = sorted_counter[0][1]\n",
        "  min_cat = sorted_counter[-1][1]\n",
        "  \n",
        "  target = min_cat + (1-ratio_down_over_up)*(max_cat - min_cat)\n",
        "  \n",
        "  for i in range(n_cat):\n",
        "    diff = int(sorted_counter[i][1] - target)\n",
        "    k = 0\n",
        "    if diff > 0:\n",
        "      rm = 0    \n",
        "      while rm <= diff:\n",
        "        if(y_train[k] == sorted_counter[i][0]):\n",
        "          X_train.pop(k)\n",
        "          y_train.pop(k)\n",
        "          count_train.pop(k)\n",
        "          rm += 1\n",
        "          k -=1\n",
        "        k += 1\n",
        "    else:\n",
        "      ad = 0\n",
        "      while ad <= -diff:\n",
        "        if(y_train[k] == sorted_counter[i][0]):\n",
        "          X_train.append(X_train[k])\n",
        "          y_train.append(y_train[k])\n",
        "          count_train.append(count_train[k])\n",
        "          ad += 1\n",
        "        k += 1\n",
        "        \n",
        "  return X_train, count_train, y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89h_sv8ENoZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_a, count_train_a, y_train_a = same_size_data(X_train_a, count_train_a, y_train_a, 0.3)\n",
        "X_train_a, count_train_a, y_train_a = shuffle(X_train_a, count_train_a, y_train_a)\n",
        "X_train_b, count_train_b, y_train_b = same_size_data(X_train_b, count_train_b, y_train_b, 0.2)\n",
        "X_train_b, count_train_b, y_train_b = shuffle(X_train_b, count_train_b, y_train_b)\n",
        "X_train_c, count_train_c, y_train_c = same_size_data(X_train_c, count_train_c, y_train_c, 0.7)\n",
        "X_train_c, count_train_c, y_train_c = shuffle(X_train_c, count_train_c, y_train_c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZNnUhQREp0P",
        "colab_type": "code",
        "outputId": "460b08b2-2a04-40a8-ac4e-8d491af55e29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "plt.figure(figsize = (12,4))\n",
        "plt.subplot(131)\n",
        "cat_a = Counter(y_train_a)\n",
        "plt.bar(list(cat_a.keys()), list(cat_a.values()))\n",
        "plt.xticks(range(len(cat_a)), label_to_number_a)\n",
        "plt.title(\"Distribition of labels in subtask A\");\n",
        "plt.subplot(132)\n",
        "cat_b = Counter(y_train_b)\n",
        "plt.bar(list(cat_b.keys()), list(cat_b.values()))\n",
        "plt.xticks(range(len(cat_b)), label_to_number_b)\n",
        "plt.title(\"Distribition of labels in subtask B\");\n",
        "plt.subplot(133)\n",
        "cat_c = Counter(y_train_c)\n",
        "plt.bar(list(cat_c.keys()), list(cat_c.values()))\n",
        "plt.xticks(range(len(cat_c)), label_to_number_c)\n",
        "plt.title(\"Distribition of labels in subtask C\");"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_tG8t1XEpzl",
        "colab_type": "text"
      },
      "source": [
        "# Baseline: Bag-of-Words model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jpALithSbb4",
        "colab_type": "text"
      },
      "source": [
        "In order to determine the ration of undersampling and oversampling, we use a Random Forest Classier in different ratio and perform a cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ek4JjdBP3U9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bag_of_words_class_tune_sampling(tweets_train, counts_train, label_train, tweets_test, count_test, label_test):\n",
        "  tweets_train = tokenizer.sequences_to_texts(tweets_train)\n",
        "  tweets_test = tokenizer.sequences_to_texts(tweets_test)\n",
        "  v = CountVectorizer()\n",
        "  transformed_train = v.fit_transform(tweets_train)\n",
        "  transformed_test = v.transform(tweets_test)\n",
        "  rf = RandomForestClassifier()\n",
        "  rf.fit(scipy.sparse.hstack([transformed_train,  scipy.sparse.coo_matrix(np.array(counts_train)[:, None])]), label_train)\n",
        "  \n",
        "  sum = 0\n",
        "  for key, elem in classification_report(rf.predict(scipy.sparse.hstack([transformed_test,  scipy.sparse.coo_matrix(np.array(count_test)[:, None])])), label_test, output_dict = True).items():\n",
        "    sum += classification_report(rf.predict(scipy.sparse.hstack([transformed_test,  scipy.sparse.coo_matrix(np.array(count_test)[:, None])])), label_test, output_dict = True)[key][\"f1-score\"]\n",
        "    \n",
        "  return sum"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9bdrHuCPyIX",
        "colab_type": "code",
        "outputId": "3857e99e-12b1-47a8-aee9-918dc762483b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "source": [
        "a_score = 0\n",
        "b_score = 0\n",
        "c_score = 0\n",
        "\n",
        "best_a_ratio = 0.1\n",
        "best_b_ratio = 0.1\n",
        "best_c_ratio = 0.1\n",
        "\n",
        "for i in range(9):\n",
        "  \n",
        "  print(\"iter {}\".format(i))\n",
        "  temp_ratio = 0.1+i*0.1\n",
        "  \n",
        "  X_train_a, X_test_a, count_train_a, count_test_a, y_train_a, y_test_a = train_test_split(build_seq(X_a), X_a.user_count, binary_labels_a, test_size=0.2)\n",
        "  X_train_b, X_test_b, count_train_b, count_test_b, y_train_b, y_test_b = train_test_split(build_seq(X_b), X_b.user_count, binary_labels_b, test_size=0.2)\n",
        "  X_train_c, X_test_c, count_train_c, count_test_c, y_train_c, y_test_c = train_test_split(build_seq(X_c), X_c.user_count, binary_labels_c, test_size=0.2)\n",
        "  \n",
        "  X_train_a, count_train_a, y_train_a = same_size_data(X_train_a, count_train_a, y_train_a, temp_ratio)\n",
        "  X_train_a, count_train_a, y_train_a = shuffle(X_train_a, count_train_a, y_train_a)\n",
        "  X_train_b, count_train_b, y_train_b = same_size_data(X_train_b, count_train_b, y_train_b, temp_ratio)\n",
        "  X_train_b, count_train_b, y_train_b = shuffle(X_train_b, count_train_b, y_train_b)\n",
        "  X_train_c, count_train_c, y_train_c = same_size_data(X_train_c, count_train_c, y_train_c, temp_ratio)\n",
        "  X_train_c, count_train_c, y_train_c = shuffle(X_train_c, count_train_c, y_train_c)\n",
        "  \n",
        "  temp_a = bag_of_words_class_tune_sampling(X_train_a, count_train_a ,y_train_a, X_test_a, count_test_a ,y_test_a)\n",
        "  temp_b = bag_of_words_class_tune_sampling(X_train_b,count_train_b ,y_train_b, X_test_b,count_test_b ,y_test_b)\n",
        "  temp_c = bag_of_words_class_tune_sampling(X_train_c,count_train_c ,y_train_c, X_test_c,count_test_c ,y_test_c)\n",
        "  \n",
        "  if(temp_a > a_score):\n",
        "    best_a_ratio = temp_ratio\n",
        "    a_score = temp_a\n",
        "  if(temp_b > b_score):\n",
        "    best_b_ratio = temp_ratio\n",
        "    b_score = temp_b\n",
        "  if(temp_c > c_score):\n",
        "    best_c_ratio = temp_ratio\n",
        "    c_score = temp_c\n",
        "    \n",
        "print(\"best_a_ratio = {}\".format(best_a_ratio))\n",
        "print(\"best_b_ratio = {}\".format(best_b_ratio))\n",
        "print(\"best_c_ratio = {}\".format(best_c_ratio))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-14617feda784>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mX_train_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_train_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_train_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mtemp_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbag_of_words_class_tune_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_train_a\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0my_train_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_test_a\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0my_test_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0mtemp_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbag_of_words_class_tune_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount_train_b\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0my_train_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount_test_b\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0my_test_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mtemp_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbag_of_words_class_tune_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_c\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount_train_c\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0my_train_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_c\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount_test_c\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0my_test_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-b224f87376dc>\u001b[0m in \u001b[0;36mbag_of_words_class_tune_sampling\u001b[0;34m(tweets_train, counts_train, label_train, tweets_test, count_test, label_test)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0msum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformed_test\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformed_test\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"f1-score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz8wXVHHRF1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bag_of_words_test(tweets_train, counts_train, label_train, tweets_test, count_test, label_test):\n",
        "  tweets_train = tokenizer.sequences_to_texts(tweets_train)\n",
        "  tweets_test = tokenizer.sequences_to_texts(tweets_test)\n",
        "  v = CountVectorizer()\n",
        "  transformed_train = v.fit_transform(tweets_train)\n",
        "  transformed_test = v.transform(tweets_test)\n",
        "  rf = RandomForestClassifier()\n",
        "  rf.fit(scipy.sparse.hstack([transformed_train,  scipy.sparse.coo_matrix(np.array(counts_train)[:, None])]), label_train)\n",
        "  print(classification_report(rf.predict(scipy.sparse.hstack([transformed_test,  scipy.sparse.coo_matrix(np.array(count_test)[:, None])])), label_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdWFGtRTEpzn",
        "colab_type": "code",
        "outputId": "dbecf597-bac7-41b3-ceae-ab0606393511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "### Baseline results\n",
        "bag_of_words_test(X_train_a, count_train_a ,y_train_a, X_test_a, count_test_a ,y_test_a)\n",
        "bag_of_words_test(X_train_b,count_train_b ,y_train_b, X_test_b,count_test_b ,y_test_b)\n",
        "bag_of_words_test(X_train_c,count_train_c ,y_train_c, X_test_c,count_test_c ,y_test_c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.69      0.55       559\n",
            "           1       0.90      0.78      0.84      2089\n",
            "\n",
            "    accuracy                           0.76      2648\n",
            "   macro avg       0.68      0.74      0.70      2648\n",
            "weighted avg       0.81      0.76      0.78      2648\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.43      0.29        46\n",
            "           1       0.97      0.91      0.94       834\n",
            "\n",
            "    accuracy                           0.89       880\n",
            "   macro avg       0.59      0.67      0.61       880\n",
            "weighted avg       0.93      0.89      0.91       880\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.78      0.81       522\n",
            "           1       0.06      0.40      0.11        10\n",
            "           2       0.64      0.59      0.61       244\n",
            "\n",
            "    accuracy                           0.72       776\n",
            "   macro avg       0.51      0.59      0.51       776\n",
            "weighted avg       0.77      0.72      0.74       776\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcq6yjP0Ep0c",
        "colab_type": "text"
      },
      "source": [
        "# Train our own embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VHWtgsHA44N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_SIZE = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbWhu7IqEp0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(data.clean)\n",
        "seq = pad_sequences(sequences)\n",
        "preprocessed = tokenizer.sequences_to_texts(seq)\n",
        "sentences = [t.split() for t in preprocessed]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk-M0ibUcgd4",
        "colab_type": "code",
        "outputId": "a3a913ec-ca1c-45fe-c512-1e3b315ed844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = FastText(sentences, size=EMBEDDING_SIZE, window=5, min_count=1, workers=4, iter = 40)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-13 15:03:55,121 : INFO : collecting all words and their counts\n",
            "2020-05-13 15:03:55,122 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2020-05-13 15:03:55,178 : INFO : PROGRESS: at sentence #10000, processed 208515 words, keeping 18006 word types\n",
            "2020-05-13 15:03:55,196 : INFO : collected 21250 word types from a corpus of 276361 raw words and 13240 sentences\n",
            "2020-05-13 15:03:55,197 : INFO : Loading a fresh vocabulary\n",
            "2020-05-13 15:03:55,242 : INFO : effective_min_count=1 retains 21250 unique words (100% of original 21250, drops 0)\n",
            "2020-05-13 15:03:55,243 : INFO : effective_min_count=1 leaves 276361 word corpus (100% of original 276361, drops 0)\n",
            "2020-05-13 15:03:55,316 : INFO : deleting the raw counts dictionary of 21250 items\n",
            "2020-05-13 15:03:55,317 : INFO : sample=0.001 downsamples 56 most-common words\n",
            "2020-05-13 15:03:55,318 : INFO : downsampling leaves estimated 210081 word corpus (76.0% of prior 276361)\n",
            "2020-05-13 15:03:55,664 : INFO : estimated required memory for 21250 words, 139446 buckets and 100 dimensions: 88250336 bytes\n",
            "2020-05-13 15:03:55,671 : INFO : resetting layer weights\n",
            "2020-05-13 15:04:00,684 : INFO : Total number of ngrams is 139446\n",
            "2020-05-13 15:04:01,881 : INFO : training model with 4 workers on 21250 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2020-05-13 15:04:02,919 : INFO : EPOCH 1 - PROGRESS: at 68.75% examples, 140441 words/s, in_qsize 8, out_qsize 0\n",
            "2020-05-13 15:04:03,149 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:03,180 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:03,209 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:03,224 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:03,226 : INFO : EPOCH - 1 : training on 276361 raw words (210027 effective words) took 1.3s, 157489 effective words/s\n",
            "2020-05-13 15:04:04,322 : INFO : EPOCH 2 - PROGRESS: at 72.48% examples, 139813 words/s, in_qsize 7, out_qsize 1\n",
            "2020-05-13 15:04:04,490 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:04,545 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:04,565 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:04,569 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:04,571 : INFO : EPOCH - 2 : training on 276361 raw words (210232 effective words) took 1.3s, 157452 effective words/s\n",
            "2020-05-13 15:04:05,699 : INFO : EPOCH 3 - PROGRESS: at 75.97% examples, 142152 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:05,869 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:05,886 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:05,902 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:05,908 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:05,909 : INFO : EPOCH - 3 : training on 276361 raw words (210206 effective words) took 1.3s, 158006 effective words/s\n",
            "2020-05-13 15:04:06,971 : INFO : EPOCH 4 - PROGRESS: at 68.75% examples, 137231 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-13 15:04:07,212 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:07,219 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:07,253 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:07,261 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:07,261 : INFO : EPOCH - 4 : training on 276361 raw words (210245 effective words) took 1.3s, 156645 effective words/s\n",
            "2020-05-13 15:04:08,274 : INFO : EPOCH 5 - PROGRESS: at 68.55% examples, 143811 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:08,545 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:08,552 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:08,595 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:08,607 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:08,608 : INFO : EPOCH - 5 : training on 276361 raw words (210218 effective words) took 1.3s, 157276 effective words/s\n",
            "2020-05-13 15:04:09,625 : INFO : EPOCH 6 - PROGRESS: at 72.19% examples, 150967 words/s, in_qsize 8, out_qsize 0\n",
            "2020-05-13 15:04:09,905 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:09,913 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:09,933 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:09,956 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:09,957 : INFO : EPOCH - 6 : training on 276361 raw words (210117 effective words) took 1.3s, 157114 effective words/s\n",
            "2020-05-13 15:04:10,999 : INFO : EPOCH 7 - PROGRESS: at 68.41% examples, 139945 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:11,221 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:11,277 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:11,286 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:11,310 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:11,311 : INFO : EPOCH - 7 : training on 276361 raw words (210177 effective words) took 1.3s, 156410 effective words/s\n",
            "2020-05-13 15:04:12,343 : INFO : EPOCH 8 - PROGRESS: at 72.19% examples, 148454 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:12,597 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:12,627 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:12,642 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:12,649 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:12,649 : INFO : EPOCH - 8 : training on 276361 raw words (209878 effective words) took 1.3s, 158001 effective words/s\n",
            "2020-05-13 15:04:13,667 : INFO : EPOCH 9 - PROGRESS: at 68.41% examples, 143142 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:13,911 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:13,962 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:13,971 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:13,999 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:14,001 : INFO : EPOCH - 9 : training on 276361 raw words (210025 effective words) took 1.3s, 156578 effective words/s\n",
            "2020-05-13 15:04:15,030 : INFO : EPOCH 10 - PROGRESS: at 68.41% examples, 141630 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:15,314 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:15,343 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:15,352 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:15,367 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:15,368 : INFO : EPOCH - 10 : training on 276361 raw words (210309 effective words) took 1.4s, 155073 effective words/s\n",
            "2020-05-13 15:04:16,439 : INFO : EPOCH 11 - PROGRESS: at 72.15% examples, 143165 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:16,693 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:16,718 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:16,755 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:16,762 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:16,763 : INFO : EPOCH - 11 : training on 276361 raw words (210165 effective words) took 1.4s, 151728 effective words/s\n",
            "2020-05-13 15:04:17,814 : INFO : EPOCH 12 - PROGRESS: at 68.41% examples, 143156 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:18,125 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:18,138 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:18,149 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:18,166 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:18,167 : INFO : EPOCH - 12 : training on 276361 raw words (209953 effective words) took 1.4s, 154321 effective words/s\n",
            "2020-05-13 15:04:19,185 : INFO : EPOCH 13 - PROGRESS: at 68.41% examples, 142832 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:19,452 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:19,502 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:19,518 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:19,530 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:19,531 : INFO : EPOCH - 13 : training on 276361 raw words (210096 effective words) took 1.4s, 154944 effective words/s\n",
            "2020-05-13 15:04:20,559 : INFO : EPOCH 14 - PROGRESS: at 64.97% examples, 134289 words/s, in_qsize 8, out_qsize 2\n",
            "2020-05-13 15:04:20,799 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:20,853 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:20,889 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:20,891 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:20,894 : INFO : EPOCH - 14 : training on 276361 raw words (210114 effective words) took 1.4s, 155434 effective words/s\n",
            "2020-05-13 15:04:21,947 : INFO : EPOCH 15 - PROGRESS: at 68.75% examples, 138615 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-13 15:04:22,192 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:22,231 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:22,235 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:22,264 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:22,265 : INFO : EPOCH - 15 : training on 276361 raw words (210157 effective words) took 1.4s, 154631 effective words/s\n",
            "2020-05-13 15:04:23,291 : INFO : EPOCH 16 - PROGRESS: at 72.19% examples, 149550 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:23,489 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:23,545 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:23,577 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:23,600 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:23,601 : INFO : EPOCH - 16 : training on 276361 raw words (210029 effective words) took 1.3s, 158531 effective words/s\n",
            "2020-05-13 15:04:24,630 : INFO : EPOCH 17 - PROGRESS: at 68.41% examples, 141442 words/s, in_qsize 8, out_qsize 0\n",
            "2020-05-13 15:04:24,864 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:24,899 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:24,928 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:24,936 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:24,937 : INFO : EPOCH - 17 : training on 276361 raw words (210151 effective words) took 1.3s, 158449 effective words/s\n",
            "2020-05-13 15:04:25,988 : INFO : EPOCH 18 - PROGRESS: at 68.41% examples, 138525 words/s, in_qsize 8, out_qsize 1\n",
            "2020-05-13 15:04:26,200 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:26,227 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:26,267 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:26,270 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:26,272 : INFO : EPOCH - 18 : training on 276361 raw words (210187 effective words) took 1.3s, 158617 effective words/s\n",
            "2020-05-13 15:04:27,322 : INFO : EPOCH 19 - PROGRESS: at 68.75% examples, 139042 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-13 15:04:27,541 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:27,562 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:27,615 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:27,633 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:27,634 : INFO : EPOCH - 19 : training on 276361 raw words (210002 effective words) took 1.3s, 155693 effective words/s\n",
            "2020-05-13 15:04:28,650 : INFO : EPOCH 20 - PROGRESS: at 68.41% examples, 143490 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:28,887 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:28,916 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:28,936 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:28,964 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:28,965 : INFO : EPOCH - 20 : training on 276361 raw words (209994 effective words) took 1.3s, 159059 effective words/s\n",
            "2020-05-13 15:04:30,069 : INFO : EPOCH 21 - PROGRESS: at 72.19% examples, 138784 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:30,295 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:30,307 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:30,320 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:30,328 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:30,329 : INFO : EPOCH - 21 : training on 276361 raw words (210296 effective words) took 1.4s, 155294 effective words/s\n",
            "2020-05-13 15:04:31,348 : INFO : EPOCH 22 - PROGRESS: at 68.41% examples, 143044 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-13 15:04:31,552 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:31,631 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:31,653 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:31,674 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:31,675 : INFO : EPOCH - 22 : training on 276361 raw words (209941 effective words) took 1.3s, 157455 effective words/s\n",
            "2020-05-13 15:04:32,753 : INFO : EPOCH 23 - PROGRESS: at 75.93% examples, 149522 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:32,921 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:32,990 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:32,998 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:33,013 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:33,014 : INFO : EPOCH - 23 : training on 276361 raw words (210245 effective words) took 1.3s, 158273 effective words/s\n",
            "2020-05-13 15:04:34,064 : INFO : EPOCH 24 - PROGRESS: at 68.41% examples, 138720 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:34,333 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:34,358 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:34,376 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:34,386 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:34,387 : INFO : EPOCH - 24 : training on 276361 raw words (210087 effective words) took 1.4s, 154194 effective words/s\n",
            "2020-05-13 15:04:35,430 : INFO : EPOCH 25 - PROGRESS: at 68.75% examples, 139445 words/s, in_qsize 7, out_qsize 2\n",
            "2020-05-13 15:04:35,632 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:35,702 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:35,710 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:35,724 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:35,725 : INFO : EPOCH - 25 : training on 276361 raw words (209804 effective words) took 1.3s, 158062 effective words/s\n",
            "2020-05-13 15:04:36,876 : INFO : EPOCH 26 - PROGRESS: at 75.93% examples, 139692 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-13 15:04:37,023 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:37,031 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:37,054 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:37,059 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:37,060 : INFO : EPOCH - 26 : training on 276361 raw words (210035 effective words) took 1.3s, 158520 effective words/s\n",
            "2020-05-13 15:04:38,108 : INFO : EPOCH 27 - PROGRESS: at 72.48% examples, 146498 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:38,331 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:38,341 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:38,396 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:38,403 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:38,404 : INFO : EPOCH - 27 : training on 276361 raw words (210117 effective words) took 1.3s, 157636 effective words/s\n",
            "2020-05-13 15:04:39,519 : INFO : EPOCH 28 - PROGRESS: at 72.19% examples, 137383 words/s, in_qsize 4, out_qsize 3\n",
            "2020-05-13 15:04:39,697 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:39,698 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:39,699 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:39,745 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:39,746 : INFO : EPOCH - 28 : training on 276361 raw words (210169 effective words) took 1.3s, 157767 effective words/s\n",
            "2020-05-13 15:04:40,806 : INFO : EPOCH 29 - PROGRESS: at 72.15% examples, 144571 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:40,994 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:41,048 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:41,063 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:41,074 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:41,075 : INFO : EPOCH - 29 : training on 276361 raw words (210048 effective words) took 1.3s, 159345 effective words/s\n",
            "2020-05-13 15:04:42,163 : INFO : EPOCH 30 - PROGRESS: at 72.19% examples, 140810 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-13 15:04:42,361 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:42,383 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:42,389 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:42,410 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:42,411 : INFO : EPOCH - 30 : training on 276361 raw words (210019 effective words) took 1.3s, 158429 effective words/s\n",
            "2020-05-13 15:04:43,431 : INFO : EPOCH 31 - PROGRESS: at 68.71% examples, 142810 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:43,669 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:43,696 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:43,728 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:43,750 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:43,751 : INFO : EPOCH - 31 : training on 276361 raw words (210002 effective words) took 1.3s, 157899 effective words/s\n",
            "2020-05-13 15:04:44,828 : INFO : EPOCH 32 - PROGRESS: at 72.19% examples, 142451 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:45,014 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:45,062 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:45,071 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:45,078 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:45,079 : INFO : EPOCH - 32 : training on 276361 raw words (209953 effective words) took 1.3s, 159574 effective words/s\n",
            "2020-05-13 15:04:46,158 : INFO : EPOCH 33 - PROGRESS: at 72.19% examples, 141635 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:46,346 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:46,401 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:46,420 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:46,432 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:46,433 : INFO : EPOCH - 33 : training on 276361 raw words (210022 effective words) took 1.3s, 156075 effective words/s\n",
            "2020-05-13 15:04:47,452 : INFO : EPOCH 34 - PROGRESS: at 68.41% examples, 142909 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:47,680 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:47,718 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:47,753 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:47,762 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:47,763 : INFO : EPOCH - 34 : training on 276361 raw words (210014 effective words) took 1.3s, 159143 effective words/s\n",
            "2020-05-13 15:04:48,817 : INFO : EPOCH 35 - PROGRESS: at 75.93% examples, 152521 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:48,966 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:49,080 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:49,082 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:49,082 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:49,085 : INFO : EPOCH - 35 : training on 276361 raw words (210040 effective words) took 1.3s, 160091 effective words/s\n",
            "2020-05-13 15:04:50,164 : INFO : EPOCH 36 - PROGRESS: at 75.93% examples, 149362 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:50,370 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:50,382 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:50,401 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:50,423 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:50,424 : INFO : EPOCH - 36 : training on 276361 raw words (209841 effective words) took 1.3s, 158245 effective words/s\n",
            "2020-05-13 15:04:51,454 : INFO : EPOCH 37 - PROGRESS: at 68.75% examples, 141478 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:51,706 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:51,714 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:51,716 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:51,754 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:51,754 : INFO : EPOCH - 37 : training on 276361 raw words (210265 effective words) took 1.3s, 159232 effective words/s\n",
            "2020-05-13 15:04:52,783 : INFO : EPOCH 38 - PROGRESS: at 72.19% examples, 148702 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:53,069 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:53,073 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:53,075 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:53,080 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:53,081 : INFO : EPOCH - 38 : training on 276361 raw words (209686 effective words) took 1.3s, 159269 effective words/s\n",
            "2020-05-13 15:04:54,109 : INFO : EPOCH 39 - PROGRESS: at 68.41% examples, 141533 words/s, in_qsize 8, out_qsize 1\n",
            "2020-05-13 15:04:54,346 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:54,368 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:54,398 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:54,412 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:54,412 : INFO : EPOCH - 39 : training on 276361 raw words (209831 effective words) took 1.3s, 158871 effective words/s\n",
            "2020-05-13 15:04:55,457 : INFO : EPOCH 40 - PROGRESS: at 68.41% examples, 139502 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-13 15:04:55,679 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-13 15:04:55,680 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-13 15:04:55,741 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-13 15:04:55,760 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-13 15:04:55,761 : INFO : EPOCH - 40 : training on 276361 raw words (210081 effective words) took 1.3s, 156902 effective words/s\n",
            "2020-05-13 15:04:55,763 : INFO : training on a 11054440 raw words (8402778 effective words) took 53.9s, 155953 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TEOurK0Ep0s",
        "colab_type": "text"
      },
      "source": [
        "### Embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgZ1yOonEp0v",
        "colab_type": "code",
        "outputId": "14a74316-4bd3-4928-bd65-130e24edf4c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Code if you want to use the GloVe Embedding \n",
        "embeddings_index = {}\n",
        "f = open(os.path.join('glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJcHeNNBh_tY",
        "colab_type": "code",
        "outputId": "22f593c6-0025-4a44-98e2-6a01f3721767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocabulary_size = len(tokenizer.word_index)\n",
        "vocabulary_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21250"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWs6h82rEp00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct the embedding matrix for \n",
        "embedding_matrix = np.zeros((vocabulary_size+1, EMBEDDING_SIZE))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    try:\n",
        "        embedding_vector = model.wv.word_vec(word) \n",
        "        #embedding_vector = embeddings_index[word] uncomment if you want to use GloVe\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        print(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn-BFddD4cjG",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9Gp6GErEp08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the layers\n",
        "\n",
        "inp = Input(shape = (max_seq,))\n",
        "\n",
        "\n",
        "y = Embedding(vocabulary_size+1, EMBEDDING_SIZE, weights = [embedding_matrix], input_length=max_seq, trainable = True)(inp)\n",
        "y = Dropout(0.5)(y)\n",
        "y = Conv1D(64, 4, activation='relu')(y)\n",
        "y = GlobalMaxPooling1D()(y)\n",
        "y = Dense(10, activation = 'relu')(y)\n",
        "\n",
        "# The output depends on the task so we define one for a binary classification and one for prediction 3 classes\n",
        "ab = Dense(1, activation = 'sigmoid')(y)\n",
        "c = Dense(3, activation = 'sigmoid')(y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yyxow9jT5Hh",
        "colab_type": "text"
      },
      "source": [
        "Task A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_IUjogBEp1B",
        "colab_type": "code",
        "outputId": "e95fc1e4-5dad-4e39-fdbb-48a37c68d12c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model_conv_a = Model(inputs = inp, outputs = ab)\n",
        "model_conv_a.compile(loss = \"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"] )\n",
        "model_conv_a.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 63)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 63, 100)           2125100   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 63, 100)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 60, 64)            25664     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 2,151,425\n",
            "Trainable params: 2,151,425\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kaestft8Ep1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_conv_a.fit(np.array(X_train_a), y_train_a, validation_data=(X_test_a,y_test_a), epochs = 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "I7NfP1CMEp1L",
        "colab_type": "code",
        "outputId": "df616f4e-683f-493c-dbed-d5068c5ce2c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "y_pred_a = model_conv_a.predict(X_test_a)\n",
        "y_pred_a = y_pred_a>0.5\n",
        "print(classification_report(y_test_a, y_pred_a, target_names= [\"OFF\",'NOT']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         OFF       0.65      0.46      0.54       838\n",
            "         NOT       0.78      0.89      0.83      1810\n",
            "\n",
            "    accuracy                           0.75      2648\n",
            "   macro avg       0.72      0.67      0.69      2648\n",
            "weighted avg       0.74      0.75      0.74      2648\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSAjStlUTyM-",
        "colab_type": "text"
      },
      "source": [
        "Task B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5ZevFQVEp1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_conv_b = Model(inputs = inp, outputs = ab)\n",
        "model_conv_b.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"] )\n",
        "model_conv_b.fit(np.array(X_train_b), y_train_b, validation_data=(X_test_b,y_test_b), epochs = 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UNQHkxvEp1X",
        "colab_type": "code",
        "outputId": "374131cd-02e0-4acd-dcfa-48ebec86d121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "y_pred_b = model_conv_b.predict(X_test_b)\n",
        "y_pred_b = y_pred_b>0.5\n",
        "print(classification_report(y_test_b, y_pred_b, target_names= [\"UNT\",'TIN']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         UNT       0.13      0.30      0.18        92\n",
            "         TIN       0.90      0.76      0.83       788\n",
            "\n",
            "    accuracy                           0.71       880\n",
            "   macro avg       0.52      0.53      0.50       880\n",
            "weighted avg       0.82      0.71      0.76       880\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVZE4NQS4S0w",
        "colab_type": "code",
        "outputId": "296c9988-cb90-4fab-9a06-382566c0e154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix((y_test_b),(y_pred_b))\n",
        "cm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 28,  64],\n",
              "       [189, 599]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK2tH93PL_J5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_conv_c = Model(inputs = inp, outputs = c)\n",
        "model_conv_c.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"] )\n",
        "dummy_labels_train = keras.utils.np_utils.to_categorical(y_train_c)\n",
        "dummy_labels_test = keras.utils.np_utils.to_categorical(y_test_c)\n",
        "model_conv_c.fit(np.array(X_train_c), dummy_labels_train, validation_data=(X_test_c,dummy_labels_test), epochs = 10, batch_size = 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZCksVP5MCO7",
        "colab_type": "code",
        "outputId": "e334d3ec-24e5-4f3b-9f42-0ecb48136125",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "y_pred_c = model_conv_c.predict(X_test_c)\n",
        "y_pred_c = np.argmax(y_pred_c,axis=1)\n",
        "print(classification_report(y_test_c, y_pred_c, target_names= [\"IND\",'OTH', \"GRP\"]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         IND       0.68      0.86      0.76       487\n",
            "         OTH       0.13      0.20      0.16        65\n",
            "         GRP       0.54      0.13      0.21       224\n",
            "\n",
            "    accuracy                           0.60       776\n",
            "   macro avg       0.45      0.40      0.38       776\n",
            "weighted avg       0.59      0.60      0.55       776\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usIYn1mo4wSH",
        "colab_type": "text"
      },
      "source": [
        "# Recurrent Neural Network - Bidirectional LSTM + CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSZ3aymfmKsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inp = Input(shape = (max_seq,))\n",
        "\n",
        "x = Embedding(vocabulary_size+1, EMBEDDING_SIZE, weights = [embedding_matrix], trainable = True)(inp)\n",
        "x = SpatialDropout1D(0.5)(x)\n",
        "x = Bidirectional(LSTM(128, return_sequences = True))(x)\n",
        "x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
        "avg_pool = GlobalAveragePooling1D()(x)\n",
        "max_pool = GlobalMaxPooling1D()(x)\n",
        "x = concatenate([avg_pool, max_pool])\n",
        "x = Dense(10, activation = \"relu\")(x)\n",
        "\n",
        "a = Dense(1, activation = \"sigmoid\")(x)\n",
        "c = Dense(3, activation = \"softmax\")(x)\n",
        "\n",
        "model_a = Model(inputs = inp, outputs = a)\n",
        "model_b = Model(inputs = inp, outputs = a)\n",
        "model_c = Model(inputs = inp, outputs = c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00wNnenJUNCr",
        "colab_type": "text"
      },
      "source": [
        "Task A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AC1T3644hmnt",
        "colab": {}
      },
      "source": [
        "model_a.compile( loss = \"binary_crossentropy\", optimizer = Adam(lr=0.001), metrics = [\"accuracy\"])\n",
        "model_a.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6IWacJ4oAnF",
        "colab_type": "code",
        "outputId": "cef99397-db53-4bee-a9fd-f00efd73ce93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "model_a.fit(np.array(X_train_a), y_train_a, validation_data=(X_test_a,y_test_a), epochs = 7)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 13367 samples, validate on 2648 samples\n",
            "Epoch 1/7\n",
            "13367/13367 [==============================] - 91s 7ms/step - loss: 0.6579 - accuracy: 0.5997 - val_loss: 0.5989 - val_accuracy: 0.6594\n",
            "Epoch 2/7\n",
            "13367/13367 [==============================] - 91s 7ms/step - loss: 0.5953 - accuracy: 0.6730 - val_loss: 0.5281 - val_accuracy: 0.7443\n",
            "Epoch 3/7\n",
            "13367/13367 [==============================] - 92s 7ms/step - loss: 0.5452 - accuracy: 0.7140 - val_loss: 0.5575 - val_accuracy: 0.6964\n",
            "Epoch 4/7\n",
            "13367/13367 [==============================] - 89s 7ms/step - loss: 0.5023 - accuracy: 0.7468 - val_loss: 0.5676 - val_accuracy: 0.7224\n",
            "Epoch 5/7\n",
            "13367/13367 [==============================] - 90s 7ms/step - loss: 0.4512 - accuracy: 0.7842 - val_loss: 0.5048 - val_accuracy: 0.7659\n",
            "Epoch 6/7\n",
            "13367/13367 [==============================] - 90s 7ms/step - loss: 0.4100 - accuracy: 0.8125 - val_loss: 0.5804 - val_accuracy: 0.7492\n",
            "Epoch 7/7\n",
            "13367/13367 [==============================] - 90s 7ms/step - loss: 0.3623 - accuracy: 0.8419 - val_loss: 0.5732 - val_accuracy: 0.7579\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f7d48778ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdLvjX06FtRa",
        "colab_type": "code",
        "outputId": "f17eeaa6-4cf8-4ca2-837c-9b2e2976b2a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "y_pred_a = model_a.predict(X_test_a)\n",
        "y_pred_a = y_pred_a>0.5\n",
        "print(classification_report(y_test_a, y_pred_a, target_names= [\"OFF\",'NOT']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         OFF       0.60      0.68      0.64       838\n",
            "         NOT       0.84      0.79      0.82      1810\n",
            "\n",
            "    accuracy                           0.76      2648\n",
            "   macro avg       0.72      0.74      0.73      2648\n",
            "weighted avg       0.77      0.76      0.76      2648\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pvfs8sTjhYrN",
        "colab_type": "code",
        "outputId": "2c0677f9-d419-4586-8851-c4d497c2bbb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix((y_test_a),(y_pred_a))\n",
        "cm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 573,  265],\n",
              "       [ 376, 1434]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn1pmtRA47dz",
        "colab_type": "text"
      },
      "source": [
        "Submission to codalab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLyu9K_f3ZFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_a[\"clean data\"] = test_data_a.tweet.apply(lambda x: clean_data(x)[0])\n",
        "sequences = tokenizer.texts_to_sequences(test_data_a[\"clean data\"])\n",
        "seq_test_a = pad_sequences(sequences, maxlen = max_seq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG1RLnf74AC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model_a.predict(seq_test_a)\n",
        "predictions = predictions > 0.5\n",
        "predictions = [number_to_label_a[int(v[0])] for v in predictions]\n",
        "test_data_a[\"predictions\"] = predictions\n",
        "submit= test_data_a[[\"id\", \"predictions\"]]\n",
        "submit.to_csv(\"submission_a.csv\", index = False, header = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54l4j-r85CFA",
        "colab_type": "text"
      },
      "source": [
        "Transfer Learning for task B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXcuuxLAojiZ",
        "colab_type": "code",
        "outputId": "b5584b7f-b2a1-4c26-87b3-87b5b683b161",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "model_b.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "model_b.fit(np.array(X_train_b), y_train_b, validation_data=(X_test_b,y_test_b), epochs = 3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5645 samples, validate on 880 samples\n",
            "Epoch 1/3\n",
            "5645/5645 [==============================] - 45s 8ms/step - loss: 0.6812 - accuracy: 0.6106 - val_loss: 0.6791 - val_accuracy: 0.6125\n",
            "Epoch 2/3\n",
            "5645/5645 [==============================] - 43s 8ms/step - loss: 0.5829 - accuracy: 0.6974 - val_loss: 0.4969 - val_accuracy: 0.7705\n",
            "Epoch 3/3\n",
            "5645/5645 [==============================] - 43s 8ms/step - loss: 0.5075 - accuracy: 0.7543 - val_loss: 0.6579 - val_accuracy: 0.6591\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f7d50f67860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh0ShMX6zM-D",
        "colab_type": "code",
        "outputId": "980655ca-b792-4073-a936-f870a8ac7e5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "y_pred_b = model_b.predict(X_test_b)\n",
        "y_pred_b = y_pred_b > 0.5\n",
        "print(classification_report(y_test_b, y_pred_b, target_names= [\"UNT\",'TIN']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         UNT       0.17      0.57      0.26        92\n",
            "         TIN       0.93      0.67      0.78       788\n",
            "\n",
            "    accuracy                           0.66       880\n",
            "   macro avg       0.55      0.62      0.52       880\n",
            "weighted avg       0.85      0.66      0.72       880\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z_PdIrE6V7l",
        "colab_type": "code",
        "outputId": "80443714-7323-4ec5-b532-b1b8ae939a8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "y_pred_b = model_b.predict(X_test_b)\n",
        "y_pred_b = y_pred_b > 0.5\n",
        "print(classification_report(y_test_b, y_pred_b, target_names= [\"UNT\",'TIN']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         UNT       0.17      0.57      0.26        92\n",
            "         TIN       0.93      0.67      0.78       788\n",
            "\n",
            "    accuracy                           0.66       880\n",
            "   macro avg       0.55      0.62      0.52       880\n",
            "weighted avg       0.85      0.66      0.72       880\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6piXeVBuiteB",
        "colab_type": "code",
        "outputId": "66f337fb-546c-4b55-da36-af266c9a3f4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cm = confusion_matrix((y_test_b), (y_pred_b))\n",
        "cm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 28,  64],\n",
              "       [189, 599]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKctmIuV5WeR",
        "colab_type": "text"
      },
      "source": [
        "Submission to codalab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdFCis_1aMSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_b[\"clean data\"] = test_data_b.tweet.apply(lambda x: clean_data(x)[0])\n",
        "sequences = tokenizer.texts_to_sequences(test_data_b[\"clean data\"])\n",
        "seq_test_b = pad_sequences(sequences, maxlen = max_seq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o82LjcBb68-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model_b.predict(seq_test_b)\n",
        "predictions = predictions > 0.5\n",
        "predictions = [number_to_label_b[int(v[0])] for v in predictions]\n",
        "test_data_b[\"predictions\"] = predictions\n",
        "submit= test_data_b[[\"id\", \"predictions\"]]\n",
        "submit.to_csv(\"submission_b.csv\", index = False, header = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv9I_inAJAvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model_b.predict(seq_test_b)\n",
        "predictions = np.argmax(predictions,axis=1)\n",
        "predictions = [number_to_label_b[v] for v in predictions]\n",
        "test_data_b[\"predictions\"] = predictions\n",
        "submit= test_data_b[[\"id\", \"predictions\"]]\n",
        "submit.to_csv(\"submission_b.csv\", index = False, header = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DClpcahLE2Y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "labels_b = pd.read_csv(\"labels-levelb.csv\", sep=\",\", header=None)\n",
        "labels_b.columns=[\"id\", \"subtask_b\"]\n",
        "\n",
        "print(classification_report(labels_b[\"subtask_b\"], predictions))\n",
        "print(confusion_matrix(labels_b[\"subtask_b\"], predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5neBwSP5RXL",
        "colab_type": "text"
      },
      "source": [
        "Transfer Learning for task C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV0bhX-Z-JyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-8-AeknoyvR",
        "colab_type": "code",
        "outputId": "c8218c84-988d-444c-b13a-02aa29f13375",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "model_c.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "dummy_labels_train = keras.utils.np_utils.to_categorical(y_train_c)\n",
        "dummy_labels_test = keras.utils.np_utils.to_categorical(y_test_c)\n",
        "model_c.fit(np.array(X_train_c), dummy_labels_train, validation_data=(X_test_c,dummy_labels_test), epochs = 2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5284 samples, validate on 776 samples\n",
            "Epoch 1/2\n",
            "5284/5284 [==============================] - 42s 8ms/step - loss: 1.0518 - accuracy: 0.4521 - val_loss: 0.9907 - val_accuracy: 0.5954\n",
            "Epoch 2/2\n",
            "5284/5284 [==============================] - 41s 8ms/step - loss: 0.9409 - accuracy: 0.5331 - val_loss: 0.9360 - val_accuracy: 0.6147\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f7d466e0b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmIFEqoGpSSI",
        "colab_type": "code",
        "outputId": "4f08bee1-f2cf-44e2-90da-d91e313e3721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "y_pred_c = model_c.predict(X_test_c)\n",
        "y_pred_c = np.argmax(y_pred_c,axis=1)\n",
        "print(classification_report(y_test_c, y_pred_c, target_names= [\"IND\",'OTH', \"GRP\"]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         IND       0.85      0.62      0.72       487\n",
            "         OTH       0.17      0.25      0.20        65\n",
            "         GRP       0.48      0.70      0.57       224\n",
            "\n",
            "    accuracy                           0.61       776\n",
            "   macro avg       0.50      0.52      0.50       776\n",
            "weighted avg       0.69      0.61      0.63       776\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE7oEhVCi0HQ",
        "colab_type": "code",
        "outputId": "acea39bf-5b80-41c4-8f16-758af2a7d6f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "cm = confusion_matrix((y_test_c), (y_pred_c))\n",
        "cm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[304,  46, 137],\n",
              "       [ 16,  16,  33],\n",
              "       [ 37,  30, 157]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBvkhBwOUYOd",
        "colab_type": "text"
      },
      "source": [
        "Predict on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjkQCdZu77Xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_c[\"clean data\"] = test_data_c.tweet.apply(lambda x: clean_data(x)[0])\n",
        "sequences = tokenizer.texts_to_sequences(test_data_c[\"clean data\"])\n",
        "seq_test_c = pad_sequences(sequences, maxlen = max_seq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv2C5l0T8z44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model_c.predict(seq_test_c)\n",
        "predictions = np.argmax(predictions,axis=1)\n",
        "predictions = [number_to_label_c[v] for v in predictions]\n",
        "test_data_c[\"predictions\"] = predictions\n",
        "submit= test_data_c[[\"id\", \"predictions\"]]\n",
        "submit.to_csv(\"submission_c.csv\", index = False, header = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igg6BPkYGFbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "labels_c = pd.read_csv(\"labels-levelc.csv\", sep=\",\", header=None)\n",
        "labels_c.columns=[\"id\", \"subtask_c\"]\n",
        "\n",
        "print(classification_report(labels_c[\"subtask_c\"], predictions))\n",
        "print(confusion_matrix(labels_c[\"subtask_c\"], predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMcyazWKQQHb",
        "colab_type": "text"
      },
      "source": [
        "Hyper-parameter tuning: Bayesian optimisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_-UQUGRj0tb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_Bayesian_Opt(params):\n",
        "  \n",
        "    ## Architecture\n",
        "    inp = Input(shape = (max_seq,))\n",
        "\n",
        "    x = Embedding(vocabulary_size+1, EMBEDDING_SIZE, weights = [embedding_matrix], trainable = True)(inp)\n",
        "    x = SpatialDropout1D(0.5)(x)\n",
        "    x = Bidirectional(LSTM(128, return_sequences = True))(x)\n",
        "    x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
        "    avg_pool = GlobalAveragePooling1D()(x)\n",
        "    max_pool = GlobalMaxPooling1D()(x)\n",
        "    x = concatenate([avg_pool, max_pool])\n",
        "    x = Dense(10, activation = \"relu\")(x)\n",
        "\n",
        "    a = Dense(1, activation = \"sigmoid\")(x)\n",
        "    model_a = Model(inputs = inp, outputs = a)\n",
        "    \n",
        "    model_a.compile( loss = \"binary_crossentropy\", optimizer = Adam(lr = params[0,0], decay = params[0,1]), metrics = [\"accuracy\"])\n",
        "    model_a.fit(np.array(X_train_a), y_train_a, validation_data=(X_test_a,y_test_a), epochs = 5)\n",
        "    y_pred_a = model_a.predict(X_test_a)\n",
        "    y_pred_a = y_pred_a>0.5\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    ##Check the accuracy after every iteration of Bayesian Optimisation\n",
        "    return f1_score(y_test_a, y_pred_a, average = \"macro\")\n",
        "  \n",
        "\n",
        "### For GPyOpt implementation details see: https://github.com/SheffieldML/GPyOpt/\n",
        "domain = [{'name': 'lr',  'type': 'continuous',  'domain': (0.0005,0.01)}, {'name': 'w_decay',  'type': 'continuous',  'domain': (0, 0.000001)}]\n",
        "opt = GPyOpt.methods.BayesianOptimization(f = train_Bayesian_Opt, domain = domain, acquisition_type ='LCB', acquisition_weight = 0.5)\n",
        "opt.run_optimization(max_iter=10)\n",
        "opt.plot_convergence()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}